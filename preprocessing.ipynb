{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d580b231e2cc954",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T19:42:09.417576Z",
     "start_time": "2025-10-19T19:42:08.018848Z"
    }
   },
   "outputs": [],
   "source": [
    "# If you want the nice notebook progress bars, ensure ipywidgets is installed once:\n",
    "!pip -q install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ffccd48aba28798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T19:42:09.434123Z",
     "start_time": "2025-10-19T19:42:09.427416Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook dir: E:\\Univer\\1A\\NLP\\YT Title\\yt-popularity\\src\\yt_popularity\n",
      "Project root: E:\\Univer\\1A\\NLP\\YT Title\\yt-popularity\n",
      "Data dir    : E:\\Univer\\1A\\NLP\\YT Title\\yt-popularity\\data exists: True\n",
      "Outputs dir : E:\\Univer\\1A\\NLP\\YT Title\\yt-popularity\\outputs exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    \"\"\"Walk up until we find a folder containing either `.git` or both `data` and `src`.\"\"\"\n",
    "    start = start.resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p/\".git\").exists() or ((p/\"data\").exists() and (p/\"src\").exists()):\n",
    "            return p\n",
    "    # fallback: if we're inside src/, use its parent\n",
    "    if start.name == \"src\":\n",
    "        return start.parent\n",
    "    if start.parent.name == \"src\":\n",
    "        return start.parent.parent\n",
    "    return start  # last resort\n",
    "\n",
    "# where am i?\n",
    "NB_DIR = Path.cwd().resolve()\n",
    "PROJ   = find_project_root(NB_DIR)\n",
    "DATA   = PROJ / \"data\"\n",
    "OUT    = PROJ / \"outputs\"\n",
    "OUT.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"Notebook dir:\", NB_DIR)\n",
    "print(\"Project root:\", PROJ)\n",
    "print(\"Data dir    :\", DATA, \"exists:\", DATA.exists())\n",
    "print(\"Outputs dir :\", OUT,  \"exists:\", OUT.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f36a1a938fe3a6e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T19:42:09.447831Z",
     "start_time": "2025-10-19T19:42:09.444224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW_VIDEOS  : E:\\Univer\\1A\\NLP\\YT Title\\yt-popularity\\data\\yt_metadata_en.jsonl.gz\n",
      "RAW_CHANNELS: E:\\Univer\\1A\\NLP\\YT Title\\yt-popularity\\data\\df_channels_en.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "# prefer exact filenames; if you use slightly different names, glob as a fallback\n",
    "RAW_VIDEOS   = DATA / \"yt_metadata_en.jsonl.gz\"\n",
    "RAW_CHANNELS = DATA / \"df_channels_en.tsv.gz\"\n",
    "\n",
    "# fallback using glob if the exact names aren't found\n",
    "if not RAW_VIDEOS.exists():\n",
    "    matches = sorted(DATA.glob(\"yt_metadata_en.jsonl*\"))\n",
    "    assert matches, \"Couldn't find yt_metadata_en.jsonl.* in data/\"\n",
    "    RAW_VIDEOS = matches[0]\n",
    "\n",
    "if not RAW_CHANNELS.exists():\n",
    "    matches = sorted(DATA.glob(\"df_channels_en.tsv*\"))\n",
    "    assert matches, \"Couldn't find df_channels_en.tsv.* in data/\"\n",
    "    RAW_CHANNELS = matches[0]\n",
    "\n",
    "print(\"RAW_VIDEOS  :\", RAW_VIDEOS)\n",
    "print(\"RAW_CHANNELS:\", RAW_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2820b95ec3c2506",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T19:42:09.464134Z",
     "start_time": "2025-10-19T19:42:09.459218Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings, re, unicodedata, pandas as pd, numpy as np\n",
    "warnings.simplefilter(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()  # enables df.progress_apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67b9b5a7cd42df05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T19:42:09.475097Z",
     "start_time": "2025-10-19T19:42:09.472344Z"
    }
   },
   "outputs": [],
   "source": [
    "TOP_P      = 0.90       # top-10% label\n",
    "CHUNKSIZE  = 8000_000    # tune for RAM\n",
    "TIME_GRAIN = \"M\"        # \"M\" month, will auto-fallback per cohort to \"Q\"\n",
    "MIN_COHORT = 1000       # below this, use quarter instead of month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff9b83caec2fc3d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T19:42:09.485306Z",
     "start_time": "2025-10-19T19:42:09.479848Z"
    }
   },
   "outputs": [],
   "source": [
    "# text cleaning helpers\n",
    "import re, unicodedata\n",
    "\n",
    "URL_RE  = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "AT_RE   = re.compile(r\"@\\w+\")\n",
    "HASH_RE = re.compile(r\"#\\w+\")\n",
    "\n",
    "def clean_minimal(text: str) -> str:\n",
    "    t = unicodedata.normalize(\"NFKC\", str(text))\n",
    "    t = URL_RE.sub(\"<URL>\", t)\n",
    "    t = AT_RE.sub(\"<USER>\", t)\n",
    "    t = HASH_RE.sub(\"<HASH>\", t)\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "def clean_for_tfidf(text: str) -> str:\n",
    "    t = clean_minimal(text).lower()\n",
    "    t = re.sub(r\"[^\\w\\s<>]\", \" \", t)\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "def first_category(x):\n",
    "    # categories can be a list or a stringified list or a plain string\n",
    "    if isinstance(x, list) and x:\n",
    "        return x[0]\n",
    "    if isinstance(x, str) and x.startswith(\"[\"):\n",
    "        import ast\n",
    "        try:\n",
    "            lst = ast.literal_eval(x)\n",
    "            return lst[0] if lst else \"Unknown\"\n",
    "        except Exception:\n",
    "            return \"Unknown\"\n",
    "    return x if isinstance(x, str) and x else \"Unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44bcb68ce1300a15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T19:42:09.680067Z",
     "start_time": "2025-10-19T19:42:09.493035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_id</th>\n",
       "      <th>subscribers_cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UC-lHJZR3Gqxm24_Vd_AJ5Yw</td>\n",
       "      <td>101000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCbCmjCuTUZos6Inko4u57UQ</td>\n",
       "      <td>60100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCpEhnqL0y41EpW2TvWAHD7Q</td>\n",
       "      <td>56018869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 channel_id  subscribers_cc\n",
       "0  UC-lHJZR3Gqxm24_Vd_AJ5Yw       101000000\n",
       "1  UCbCmjCuTUZos6Inko4u57UQ        60100000\n",
       "2  UCpEhnqL0y41EpW2TvWAHD7Q        56018869"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_channels_en.tsv.gz has channel subscribers\n",
    "usecols = [\"channel\", \"subscribers_cc\"]\n",
    "chan = pd.read_csv(RAW_CHANNELS, sep=\"\\t\", usecols=usecols)\n",
    "chan = chan.rename(columns={\"channel\": \"channel_id\"})\n",
    "chan[\"channel_id\"] = chan[\"channel_id\"].astype(\"string\")\n",
    "chan.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "215579f1731fec74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T20:12:54.714510Z",
     "start_time": "2025-10-19T20:12:53.582764Z"
    }
   },
   "outputs": [],
   "source": [
    "# install once if needed\n",
    "!pip -q install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf49d3910d3836ba",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-10-19T20:13:00.603208Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830d7d4a2386450eb70896ec256c5c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage 1: extracting shards: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SHARDS = OUT / \"stage1_shards\"\n",
    "SHARDS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "keep_cols = [\"display_id\",\"title\",\"categories\",\"channel_id\",\n",
    "             \"upload_date\",\"crawl_date\",\"view_count\"]\n",
    "\n",
    "def parse_dt(x):\n",
    "    return pd.to_datetime(x, errors=\"coerce\", utc=True)\n",
    "\n",
    "def make_size_bin(df):\n",
    "    subs = df[\"subscribers_cc\"].fillna(-1)\n",
    "    has  = subs >= 0\n",
    "    df[\"size_bin\"] = -1\n",
    "    if has.any():\n",
    "        df.loc[has, \"size_bin\"] = pd.qcut(\n",
    "            subs[has], q=10, labels=False, duplicates=\"drop\"\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# write skinny shards: one Parquet per chunk\n",
    "shard_idx = 0\n",
    "for chunk in tqdm(\n",
    "    pd.read_json(RAW_VIDEOS, lines=True, chunksize=CHUNKSIZE,\n",
    "                 dtype={\"channel_id\":\"string\"}, convert_dates=False),\n",
    "    desc=\"Stage 1: extracting shards\"\n",
    "):\n",
    "    chunk = chunk[[c for c in keep_cols if c in chunk.columns]]\n",
    "    chunk = chunk.dropna(subset=[\"title\",\"view_count\",\"upload_date\",\"crawl_date\"])\n",
    "\n",
    "    # dates, age, vpd\n",
    "    chunk[\"upload_date\"] = chunk[\"upload_date\"].map(parse_dt)\n",
    "    chunk[\"crawl_date\"]  = chunk[\"crawl_date\"].map(parse_dt)\n",
    "    chunk = chunk[chunk[\"upload_date\"].notna() & chunk[\"crawl_date\"].notna()]\n",
    "    age = (chunk[\"crawl_date\"] - chunk[\"upload_date\"]).dt.days.clip(lower=1)\n",
    "    vpd = (chunk[\"view_count\"] / age.replace(0,1))\n",
    "    chunk[\"vpd\"] = vpd.clip(upper=vpd.quantile(0.999))\n",
    "\n",
    "    # cohorts\n",
    "    chunk = chunk.merge(chan, on=\"channel_id\", how=\"left\")\n",
    "    chunk = make_size_bin(chunk)\n",
    "    ud_naive = chunk[\"upload_date\"].dt.tz_localize(None)\n",
    "    chunk[\"upload_period\"] = ud_naive.dt.strftime(\"%Y-%m\")  # month as string\n",
    "    counts = chunk.groupby([\"categories\",\"size_bin\",\"upload_period\"])[\"vpd\"].transform(\"size\")\n",
    "    need_q = counts < MIN_COHORT\n",
    "    if need_q.any():\n",
    "        chunk.loc[need_q, \"upload_period\"] = ud_naive[need_q].dt.to_period(\"Q\").astype(str)\n",
    "\n",
    "    # normalize category to single string\n",
    "    chunk[\"category_1\"] = chunk[\"categories\"].map(first_category)\n",
    "\n",
    "    # title variants\n",
    "    chunk[\"title_min\"]   = chunk[\"title\"].map(clean_minimal)\n",
    "    chunk[\"title_tfidf\"] = chunk[\"title\"].map(clean_for_tfidf)\n",
    "\n",
    "    skinny = chunk[[\n",
    "        \"display_id\",\"title\",\"title_min\",\"title_tfidf\",\n",
    "        \"vpd\",\"category_1\",\"size_bin\",\"upload_period\"\n",
    "    ]].copy()\n",
    "\n",
    "    shard_path = SHARDS / f\"part_{shard_idx:05d}.parquet\"\n",
    "    skinny.to_parquet(shard_path, index=False)\n",
    "    shard_idx += 1\n",
    "\n",
    "len(list(SHARDS.glob(\"part_*.parquet\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b840154029359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a2776466bc4665a9df80792a3fd695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c214a71d06f45f0b6a2d925086a5c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "sql(): incompatible function arguments. The following argument types are supported:\n    1. (self: _duckdb.DuckDBPyConnection, query: object, *, alias: str = '', params: object = None) -> _duckdb.DuckDBPyRelation\n\nInvoked with: <_duckdb.DuckDBPyConnection object at 0x0000026A5FD78F70>, 'CREATE OR REPLACE TABLE final AS SELECT *, (pctl >= ?)::INT AS label FROM labeled;', [0.9]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     12\u001b[39m db.sql(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[33m    CREATE OR REPLACE TABLE labeled AS\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[33m    SELECT\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m \u001b[33m    FROM vids;\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[33m\"\"\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# label = top-10% (>= TOP_P)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCREATE OR REPLACE TABLE final AS SELECT *, (pctl >= ?)::INT AS label FROM labeled;\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mTOP_P\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# sanity peek\u001b[39;00m\n\u001b[32m     28\u001b[39m db.sql(\u001b[33m\"\u001b[39m\u001b[33mSELECT COUNT(*) AS n, AVG(label)::DOUBLE AS pos_rate FROM final;\u001b[39m\u001b[33m\"\u001b[39m).df()\n",
      "\u001b[31mTypeError\u001b[39m: sql(): incompatible function arguments. The following argument types are supported:\n    1. (self: _duckdb.DuckDBPyConnection, query: object, *, alias: str = '', params: object = None) -> _duckdb.DuckDBPyRelation\n\nInvoked with: <_duckdb.DuckDBPyConnection object at 0x0000026A5FD78F70>, 'CREATE OR REPLACE TABLE final AS SELECT *, (pctl >= ?)::INT AS label FROM labeled;', [0.9]"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "db = duckdb.connect(database=\":memory:\")\n",
    "\n",
    "db.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE vids AS\n",
    "    SELECT *\n",
    "    FROM read_parquet('{(SHARDS/'part_*.parquet').as_posix()}');\n",
    "\"\"\")\n",
    "\n",
    "# compute percent_rank within each cohort\n",
    "db.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE labeled AS\n",
    "    SELECT\n",
    "        display_id, title, title_min, title_tfidf,\n",
    "        vpd, category_1, size_bin, upload_period,\n",
    "        percent_rank() OVER (\n",
    "            PARTITION BY category_1, size_bin, upload_period\n",
    "            ORDER BY vpd\n",
    "        ) AS pctl\n",
    "    FROM vids;\n",
    "\"\"\")\n",
    "\n",
    "# # label = top-10% (>= TOP_P)\n",
    "# db.sql(\"CREATE OR REPLACE TABLE final AS SELECT *, (pctl >= ?)::INT AS label FROM labeled;\", params=[TOP_P])\n",
    "\n",
    "# # sanity peek\n",
    "# db.sql(\"SELECT COUNT(*) AS n, AVG(label)::DOUBLE AS pos_rate FROM final;\").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90070e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2686a5f899442dd8eb18c4ae81ff697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>pos_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72924140</td>\n",
       "      <td>0.100069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          n  pos_rate\n",
       "0  72924140  0.100069"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label = top-10% (>= TOP_P)\n",
    "db.sql(\"CREATE OR REPLACE TABLE final AS SELECT *, (pctl >= ?)::INT AS label FROM labeled;\", params=[TOP_P])\n",
    "\n",
    "# sanity peek\n",
    "db.sql(\"SELECT COUNT(*) AS n, AVG(label)::DOUBLE AS pos_rate FROM final;\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f46ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to out\n",
    "LABELED = OUT / \"labeled_all.parquet\"\n",
    "db.sql(f\"COPY final TO '{LABELED.as_posix()}' (FORMAT PARQUET);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ed35b565bf554d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowMemoryError",
     "evalue": "malloc of size 17179869184 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# read back with pandas to split\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLABELED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpyarrow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.shape, \u001b[38;5;28mround\u001b[39m(df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].mean(), \u001b[32m3\u001b[39m))\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Univer\\1A\\NLP\\YT Title\\yt-popularity\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Univer\\1A\\NLP\\YT Title\\yt-popularity\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:279\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings():\n\u001b[32m    274\u001b[39m     filterwarnings(\n\u001b[32m    275\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    276\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmake_block is deprecated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    277\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    278\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     result = \u001b[43marrow_table_to_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mto_pandas_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mto_pandas_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    286\u001b[39m     result = result._as_manager(\u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Univer\\1A\\NLP\\YT Title\\yt-popularity\\.venv\\Lib\\site-packages\\pandas\\io\\_util.py:93\u001b[39m, in \u001b[36marrow_table_to_pandas\u001b[39m\u001b[34m(table, dtype_backend, null_to_int64, to_pandas_kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m df = \u001b[43mtable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtypes_mapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypes_mapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mto_pandas_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Univer\\1A\\NLP\\YT Title\\yt-popularity\\.venv\\Lib\\site-packages\\pyarrow\\array.pxi:1020\u001b[39m, in \u001b[36mpyarrow.lib._PandasConvertible.to_pandas\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Univer\\1A\\NLP\\YT Title\\yt-popularity\\.venv\\Lib\\site-packages\\pyarrow\\table.pxi:5177\u001b[39m, in \u001b[36mpyarrow.lib.Table._to_pandas\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Univer\\1A\\NLP\\YT Title\\yt-popularity\\.venv\\Lib\\site-packages\\pyarrow\\pandas_compat.py:806\u001b[39m, in \u001b[36mtable_to_dataframe\u001b[39m\u001b[34m(options, table, categories, ignore_metadata, types_mapper)\u001b[39m\n\u001b[32m    803\u001b[39m columns = _deserialize_column_index(table, all_columns, column_indexes)\n\u001b[32m    805\u001b[39m column_names = table.column_names\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m result = \u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtable_to_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mext_columns_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _pandas_api.is_ge_v3():\n\u001b[32m    809\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_dataframe_from_blocks\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Univer\\1A\\NLP\\YT Title\\yt-popularity\\.venv\\Lib\\site-packages\\pyarrow\\table.pxi:4103\u001b[39m, in \u001b[36mpyarrow.lib.table_to_blocks\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Univer\\1A\\NLP\\YT Title\\yt-popularity\\.venv\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowMemoryError\u001b[39m: malloc of size 17179869184 failed"
     ]
    }
   ],
   "source": [
    "# read back with pandas to split\n",
    "df = pd.read_parquet(LABELED, engine=\"pyarrow\")\n",
    "print(df.shape, round(df['label'].mean(), 3))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, temp_df = train_test_split(df, test_size=0.30, stratify=df[\"label\"], random_state=42)\n",
    "val_df, test_df   = train_test_split(temp_df, test_size=0.50, stratify=temp_df[\"label\"], random_state=42)\n",
    "\n",
    "SPLITS = OUT / \"splits\"\n",
    "SPLITS.mkdir(parents=True, exist_ok=True)\n",
    "train_df.to_parquet(SPLITS/\"train.parquet\", index=False)\n",
    "val_df.to_parquet(  SPLITS/\"val.parquet\",   index=False)\n",
    "test_df.to_parquet( SPLITS/\"test.parquet\",  index=False)\n",
    "\n",
    "manifest = {\n",
    "    \"top_percentile\": TOP_P,\n",
    "    \"time_grain\": \"month_with_fallback_to_quarter\",\n",
    "    \"min_cohort\": MIN_COHORT,\n",
    "    \"rows_total\": int(df.shape[0]),\n",
    "    \"rows_train\": int(train_df.shape[0]),\n",
    "    \"rows_val\":   int(val_df.shape[0]),\n",
    "    \"rows_test\":  int(test_df.shape[0]),\n",
    "}\n",
    "pd.Series(manifest).to_json(SPLITS/\"manifest.json\", indent=2)\n",
    "manifest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a8ed438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13aa418bbe647c0ad0eb9a42dd16bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      n_all\n",
      "0  72924140\n",
      "    n_2017p\n",
      "0  40485011\n"
     ]
    }
   ],
   "source": [
    "#Remove videos before 2017\n",
    "\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "\n",
    "IN  = Path(r\"labeled_all.parquet\")         # <-- put your path\n",
    "OUT = Path(r\"labeled_2017plus.parquet\")  # new file\n",
    "\n",
    "db = duckdb.connect(database=\":memory:\")\n",
    "\n",
    "db.sql(f\"\"\"\n",
    "  COPY (\n",
    "    SELECT *\n",
    "    FROM read_parquet('{IN.as_posix()}')\n",
    "    WHERE TRY_CAST(substr(upload_period, 1, 4) AS INTEGER) >= 2017\n",
    "  ) TO '{OUT.as_posix()}' (FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "# (Optional) quick sanity counts (tiny results, safe to fetch)\n",
    "print(db.sql(f\"SELECT COUNT(*) AS n_all FROM read_parquet('{IN.as_posix()}')\").df())\n",
    "print(db.sql(f\"SELECT COUNT(*) AS n_2017p FROM read_parquet('{OUT.as_posix()}')\").df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a6424f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most recent year:\n",
      "    most_recent_year\n",
      "0              2019\n",
      "\n",
      "Number of unique categories:\n",
      "    n_categories\n",
      "0            15\n"
     ]
    }
   ],
   "source": [
    " # most recent year\n",
    "recent_year = db.sql(f\"\"\"\n",
    "    SELECT MAX(CAST(substr(upload_period,1,4) AS INTEGER)) AS most_recent_year\n",
    "    FROM read_parquet('{OUT.as_posix()}');\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"Most recent year:\\n\", recent_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db54460e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     n_2019\n",
      "0  12723002\n"
     ]
    }
   ],
   "source": [
    "n_2019 = db.sql(f\"\"\"\n",
    "    SELECT COUNT(*) AS n_2019\n",
    "    FROM read_parquet('{OUT.as_posix()}')\n",
    "    WHERE CAST(substr(upload_period, 1, 4) AS INTEGER) = 2019;\n",
    "\"\"\").df()\n",
    "\n",
    "print(n_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec4162e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories: 15\n",
      "\n",
      "Category names:\n",
      "- Autos & Vehicles\n",
      "- Comedy\n",
      "- Education\n",
      "- Entertainment\n",
      "- Film & Animation\n",
      "- Gaming\n",
      "- Howto & Style\n",
      "- Music\n",
      "- News & Politics\n",
      "- Nonprofits & Activism\n",
      "- People & Blogs\n",
      "- Pets & Animals\n",
      "- Science & Technology\n",
      "- Sports\n",
      "- Travel & Events\n"
     ]
    }
   ],
   "source": [
    "categories = db.sql(f\"\"\"\n",
    "    SELECT DISTINCT category_1\n",
    "    FROM read_parquet('{OUT.as_posix()}')\n",
    "    WHERE category_1 IS NOT NULL AND category_1 <> 'Unknown'\n",
    "    ORDER BY category_1;\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"Number of categories: {len(categories)}\\n\")\n",
    "print(\"Category names:\")\n",
    "for name in categories[\"category_1\"]:\n",
    "    print(\"-\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0971f699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos in 2017: 12,486,216\n",
      "\n",
      "Videos per category:\n",
      "- Gaming — 2,478,332\n",
      "- Entertainment — 2,176,253\n",
      "- People & Blogs — 1,411,465\n",
      "- Music — 1,320,469\n",
      "- News & Politics — 1,290,147\n",
      "- Howto & Style — 702,145\n",
      "- Sports — 687,901\n",
      "- Education — 659,990\n",
      "- Film & Animation — 430,470\n",
      "- Science & Technology — 390,602\n",
      "- Autos & Vehicles — 345,724\n",
      "- Comedy — 212,663\n",
      "- Travel & Events — 160,395\n",
      "- Nonprofits & Activism — 118,499\n",
      "- Pets & Animals — 101,161\n"
     ]
    }
   ],
   "source": [
    "YEAR = 2017\n",
    "where_clauses = [\n",
    "    \"category_1 IS NOT NULL\",\n",
    "    \"category_1 <> 'Unknown'\",\n",
    "]\n",
    "if YEAR is not None:\n",
    "    where_clauses.append(f\"CAST(substr(upload_period,1,4) AS INTEGER) = {YEAR}\")\n",
    "\n",
    "where_sql = \" AND \".join(where_clauses)\n",
    "\n",
    "# Get totals and per-category counts\n",
    "total_df = db.sql(f\"\"\"\n",
    "    SELECT COUNT(*) AS n\n",
    "    FROM read_parquet('{OUT.as_posix()}')\n",
    "    WHERE {where_sql};\n",
    "\"\"\").df()\n",
    "\n",
    "by_cat_df = db.sql(f\"\"\"\n",
    "    SELECT\n",
    "        category_1 AS category,\n",
    "        COUNT(*)   AS n\n",
    "    FROM read_parquet('{OUT.as_posix()}')\n",
    "    WHERE {where_sql}\n",
    "    GROUP BY category_1\n",
    "    ORDER BY n DESC;\n",
    "\"\"\").df()\n",
    "\n",
    "scope = f\"in {YEAR}\" if YEAR is not None else \"overall (2017+ subset)\"\n",
    "print(f\"Total videos {scope}: {total_df.loc[0, 'n']:,}\\n\")\n",
    "\n",
    "print(\"Videos per category:\")\n",
    "for _, row in by_cat_df.iterrows():\n",
    "    print(f\"- {row['category']} — {row['n']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a1aa81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f465b55e044095957217fd16b7f9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in 2019 before filtering: 12,723,002\n",
      "Rows kept after filtering:     10,866,563\n",
      "Eligible categories (>= 300,000 vids, excluding 'Music'):\n",
      "- News & Politics — 2,566,824\n",
      "- Entertainment — 2,226,959\n",
      "- Gaming — 2,014,444\n",
      "- People & Blogs — 1,107,551\n",
      "- Sports — 683,717\n",
      "- Education — 604,234\n",
      "- Howto & Style — 594,860\n",
      "- Film & Animation — 392,300\n",
      "- Science & Technology — 364,092\n",
      "- Autos & Vehicles — 311,582\n"
     ]
    }
   ],
   "source": [
    "OUT_FILE  = Path(r\"labeled_2019.parquet\")\n",
    "THRESHOLD = 300_000  # min videos per category\n",
    "db.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW v2019 AS\n",
    "    SELECT *\n",
    "    FROM read_parquet('{OUT.as_posix()}')\n",
    "    WHERE TRY_CAST(substr(upload_period,1,4) AS INTEGER) = 2019;\n",
    "\"\"\")\n",
    "\n",
    "# Find categories meeting the threshold (excluding 'Music')\n",
    "db.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE eligible AS\n",
    "    SELECT category_1, COUNT(*) AS n\n",
    "    FROM v2019\n",
    "    WHERE category_1 IS NOT NULL AND category_1 <> 'Music'\n",
    "    GROUP BY category_1\n",
    "    HAVING COUNT(*) >= {THRESHOLD};\n",
    "\"\"\")\n",
    "\n",
    "# Keep only rows in eligible categories (and not 'Music'), then write out\n",
    "db.sql(f\"\"\"\n",
    "    COPY (\n",
    "      SELECT v.*\n",
    "      FROM v2019 v\n",
    "      INNER JOIN eligible e\n",
    "      ON v.category_1 = e.category_1\n",
    "    ) TO '{OUT_FILE.as_posix()}'\n",
    "    (FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "# summary \n",
    "before = db.sql(\"SELECT COUNT(*) AS n FROM v2019;\").df().iloc[0,0]\n",
    "after  = db.sql(f\"SELECT COUNT(*) AS n FROM read_parquet('{OUT_FILE.as_posix()}');\").df().iloc[0,0]\n",
    "cats   = db.sql(f\"SELECT category_1, n FROM eligible ORDER BY n DESC;\").df()\n",
    "\n",
    "print(f\"Rows in 2019 before filtering: {before:,}\")\n",
    "print(f\"Rows kept after filtering:     {after:,}\")\n",
    "print(f\"Eligible categories (>= {THRESHOLD:,} vids, excluding 'Music'):\")\n",
    "for _, r in cats.iterrows():\n",
    "    print(f\"- {r['category_1']} — {int(r['n']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e486c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = Path('labeled_2019.parquet')\n",
    "src = latest.as_posix()\n",
    "db = duckdb.connect(database=\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9adc5375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display_id:\n",
      "  0    Wf66Rb6fE8w\n",
      "Name: display_id, dtype: object\n",
      "\n",
      "title:\n",
      "  0    OMG! Salman Khan's Being Human CEO Controversy...\n",
      "Name: title, dtype: object\n",
      "\n",
      "title_min:\n",
      "  0    OMG! Salman Khan's Being Human CEO Controversy...\n",
      "Name: title_min, dtype: object\n",
      "\n",
      "title_tfidf:\n",
      "  0    omg salman khan s being human ceo controversy ...\n",
      "Name: title_tfidf, dtype: object\n",
      "\n",
      "vpd:\n",
      "  0    2.753304\n",
      "Name: vpd, dtype: float64\n",
      "\n",
      "category_1:\n",
      "  0    Entertainment\n",
      "Name: category_1, dtype: object\n",
      "\n",
      "size_bin:\n",
      "  0    7\n",
      "Name: size_bin, dtype: int64\n",
      "\n",
      "upload_period:\n",
      "  0    2019-03\n",
      "Name: upload_period, dtype: object\n",
      "\n",
      "pctl:\n",
      "  0    0.223648\n",
      "Name: pctl, dtype: float64\n",
      "\n",
      "label:\n",
      "  0    0\n",
      "Name: label, dtype: int32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# one example \n",
    "example = db.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_parquet('{src}')\n",
    "    ORDER BY random()\n",
    "    LIMIT 1;\n",
    "\"\"\").df()\n",
    "\n",
    "for col, val in example.items():\n",
    "    print(f\"{col}:\\n  {val}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ee30ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langid in e:\\univer\\1a\\nlp\\yt title\\yt-popularity\\.venv\\lib\\site-packages (1.1.6)\n",
      "Requirement already satisfied: numpy in e:\\univer\\1a\\nlp\\yt title\\yt-popularity\\.venv\\lib\\site-packages (from langid) (2.3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deb749a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247fa0d3167f4d2c8596421becf1a504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kept english titles: 7,701,389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_duckdb.DuckDBPyConnection at 0x247183db5b0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb, langid, pyarrow as pa\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "src  = Path(r\"labeled_2019.parquet\").as_posix()\n",
    "dest = Path(r\"labeled_all_english.parquet\").as_posix()\n",
    "\n",
    "# clean up old table\n",
    "db.sql(\"DROP TABLE IF EXISTS keep_ids;\")\n",
    "\n",
    "\n",
    "\n",
    "rel = db.sql(f\"\"\"\n",
    "    SELECT display_id, title\n",
    "    FROM read_parquet('{src}')\n",
    "    WHERE title IS NOT NULL AND length(title) >= 3\n",
    "\"\"\")\n",
    "\n",
    "obj = rel.arrow()\n",
    "reader = pa.RecordBatchReader.from_batches(obj.schema, obj.to_batches()) if isinstance(obj, pa.Table) else obj\n",
    "\n",
    "def is_english(t: str) -> bool:\n",
    "    lang, score = langid.classify(t.replace(\"\\n\", \" \"))\n",
    "    return lang == \"en\"\n",
    "\n",
    "\n",
    "keep_ids = []\n",
    "for batch in reader:\n",
    "    tbl = pa.Table.from_batches([batch]) if isinstance(batch, pa.RecordBatch) else batch\n",
    "    for vid, t in zip(tbl.column(\"display_id\").to_pylist(),\n",
    "                      tbl.column(\"title\").to_pylist()):\n",
    "        if t and is_english(t):\n",
    "            keep_ids.append(vid)\n",
    "\n",
    "print(f\"kept english titles: {len(keep_ids):,}\")\n",
    "\n",
    "if not keep_ids:\n",
    "    raise RuntimeError(\"No rows passed the English filter — relax the predicate or check the input.\")\n",
    "\n",
    "# register kept IDs and write filtered parquet\n",
    "keep_df = pd.DataFrame({\"display_id\": keep_ids})\n",
    "db.register(\"keep_ids_df\", keep_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454d93cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         n\n",
      "0  7701389\n"
     ]
    }
   ],
   "source": [
    "dest = Path(r\"labeled_2019_english.parquet\").as_posix()\n",
    "\n",
    "db.sql(f\"\"\"\n",
    "  COPY (\n",
    "    SELECT v.*\n",
    "    FROM read_parquet('{src}') AS v\n",
    "    JOIN keep_ids_df USING(display_id)\n",
    "  ) TO '{dest}' (FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "print(db.sql(f\"SELECT COUNT(*) AS n FROM read_parquet('{dest}')\").df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a40950a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display_id:\n",
      "  0    zrEEvp3wI7o\n",
      "1    bTBgTeCQ1B4\n",
      "2    sDa1IkBoDCY\n",
      "Name: display_id, dtype: object\n",
      "\n",
      "title:\n",
      "  0    SAINA NEHWAL & MEENAKSHI DIXITON RAMP FOR KANC...\n",
      "1             How To Draw The Loot Llama From Fortnite\n",
      "2             Ping G400 LST vs Ping G410+ | TEST DRIVE\n",
      "Name: title, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# one example \n",
    "example = db.sql(f\"\"\"\n",
    "  SELECT display_id, title\n",
    "  FROM read_parquet('{dest}')\n",
    "  ORDER BY random()\n",
    "  LIMIT 3\n",
    "\"\"\").df()\n",
    "\n",
    "for col, val in example.items():\n",
    "    print(f\"{col}:\\n  {val}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9c6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cohorts: 859\n",
      "\n",
      "Example video:\n",
      "========================================\n",
      "display_id:\n",
      "  jxe6sjvWtP4\n",
      "\n",
      "title:\n",
      "  Maibatsu Revolution SG-RX Customization & Showcase - Grand Theft Auto 5 Mods\n",
      "\n",
      "title_min:\n",
      "  Maibatsu Revolution SG-RX Customization & Showcase - Grand Theft Auto 5 Mods\n",
      "\n",
      "title_tfidf:\n",
      "  maibatsu revolution sg rx customization showcase grand theft auto 5 mods\n",
      "\n",
      "vpd:\n",
      "  14.27946127946128\n",
      "\n",
      "category_1:\n",
      "  Gaming\n",
      "\n",
      "size_bin:\n",
      "  6\n",
      "\n",
      "upload_period:\n",
      "  2019-01\n",
      "\n",
      "pctl:\n",
      "  0.31518296638302745\n",
      "\n",
      "label:\n",
      "  0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of cohorts\n",
    "n_coh = db.sql(f\"\"\"\n",
    "    SELECT COUNT(*) AS n_cohorts\n",
    "    FROM (\n",
    "        SELECT DISTINCT category_1, size_bin, upload_period\n",
    "        FROM read_parquet('{dest}')\n",
    "    )\n",
    "\"\"\").df().iloc[0, 0]\n",
    "\n",
    "print(f\"Number of cohorts: {n_coh:,}\\n\")\n",
    "\n",
    "# example\n",
    "ex = db.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_parquet('{dest}')\n",
    "    ORDER BY random()\n",
    "    LIMIT 1\n",
    "\"\"\").df()\n",
    "\n",
    "if ex.empty:\n",
    "    print(\"No rows found in the dataset.\")\n",
    "else:\n",
    "    row = ex.iloc[0].to_dict()\n",
    "    print(\"Example video:\\n\" + \"=\"*40)\n",
    "    for k, v in row.items():\n",
    "        print(f\"{k}:\\n  {v}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529df3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4975849d4449b0aac389953fc3b6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5bc46864ff4f199030a3d3add2b910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FILE_IN  = Path(\"labeled_2019_english.parquet\").as_posix()        \n",
    "FILE_OUT = Path(\"labeled_2019_english_reranked.parquet\").as_posix() # output with new pctl label\n",
    "TOP_P = 0.80  # top 20\n",
    "\n",
    "\n",
    "db.sql(f\"CREATE OR REPLACE VIEW en AS SELECT * FROM read_parquet('{FILE_IN}');\")\n",
    "\n",
    "# Recompute percentile within cohorts and write out\n",
    "db.sql(\"\"\"\n",
    "  CREATE OR REPLACE TABLE en_labeled AS\n",
    "  SELECT\n",
    "    display_id, title, title_min, title_tfidf,\n",
    "    vpd, category_1, size_bin, upload_period,\n",
    "    percent_rank() OVER (\n",
    "      PARTITION BY category_1, size_bin, upload_period\n",
    "      ORDER BY vpd NULLS LAST\n",
    "    ) AS pctl\n",
    "  FROM en\n",
    "  WHERE vpd IS NOT NULL;\n",
    "\"\"\")\n",
    "\n",
    "db.sql(\"\"\"\n",
    "  CREATE OR REPLACE TABLE en_final AS\n",
    "  SELECT *,\n",
    "         CAST(pctl >= ? AS INTEGER) AS label\n",
    "  FROM en_labeled;\n",
    "\"\"\", params=[TOP_P])\n",
    "\n",
    "db.sql(f\"COPY en_final TO '{FILE_OUT}' (FORMAT PARQUET);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42eccac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
